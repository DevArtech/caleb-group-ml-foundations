{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Adam Haile - 2/20/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's check out decision trees on a different dataset. For this one, we'll use another common toy dataset. The **Titanic dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these if you don't have them.\n",
    "# %pip install numpy matplotlib scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Titanic-Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our data, we can see a good amount of features, and our target variable. In this case, we want to use our features to determine **whether or not a passenger would survive**. \n",
    "\n",
    "Let's extract out some more numerical, and relevant features for our model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "df = df[features + [\"Survived\"]]\n",
    "\n",
    "# Handle missing values (fill age with median)\n",
    "df = df.assign(Age=df[\"Age\"].fillna(df[\"Age\"].median()))\n",
    "\n",
    "# Encode categorical variables (Sex: male=1, female=0)\n",
    "df[\"Sex\"] = LabelEncoder().fit_transform(df[\"Sex\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what we just did.\n",
    "1. We extracted out 6 features for us to use.\n",
    "    * Pclass: The class of the ticket of the passenger (1st, 2nd, 3rd)\n",
    "    * Sex: The gender of the passenger (male, female)\n",
    "    * Age: The age of the passenger\n",
    "    * SibSp: Number of siblings/spouses that were also aboard the Titanic\n",
    "    * Parch: Number of parameters/children that were also aboard the Titanic\n",
    "    * Fare: The cost of their ticket.\n",
    "\n",
    "    We also add back on our target variable (Survived)\n",
    "\n",
    "2. We fill in any missing data that we have in our Age column with the median age (a very naive approach, but is fine enough for our example).\n",
    "3. We convert our categorical values in sex to one-hot encoded values (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll split our data and train a decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features]\n",
    "y = df[\"Survived\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's see how well it does at predicting the survival of passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "f\"{accuracy * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We can see that our decision tree gets almost 80% on predicting survival rate! Let's visualize some of our results now and interpret how we got here. Let's start with seeing what our trained decision tree looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(clf, feature_names=X.columns, class_names=[\"Not Survived\", \"Survived\"], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret our tree by reading the values on the nodes.\n",
    "\n",
    "The first value of the non-leaf nodes tell us the feature the node analyzes, and the threshold of it. Because we are working with numerical values, all decisions are generally split on bases of less than some trained value.\n",
    "Below that we see the gini index score from this node. This tells us how purely the node split the data. Remember that higher gini index scores, the more pure the split was (with 0.5 being a max Gini index value).\n",
    "We can also see the number of samples that the node split on, and some example values that would be at the feature.\n",
    "\n",
    "The leaf nodes show us as well the exact class that they predict an input results in when the data reaches to that point in the tree.\n",
    "\n",
    "Let's check out another way we can visualize decision trees. We can look at the **decision boundaries** of the nodes in **feature space**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll only use the first two features for visualization.\n",
    "\n",
    "# Select only the first two features for visualization\n",
    "X_vis = X_train.iloc[:, 3:5].values\n",
    "y_vis = y_train.values\n",
    "\n",
    "# Train a new decision tree using only these two features\n",
    "clf_vis = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_vis.fit(X_vis, y_vis)\n",
    "\n",
    "# Generate a mesh grid for plotting decision boundaries\n",
    "x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
    "y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "# Predict class for each point in the grid\n",
    "Z = clf_vis.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(clf_vis, feature_names=X_train.columns[3:5], class_names=[\"Not Survived\", \"Survived\"], filled=True)\n",
    "plt.show()\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "scatter = plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_vis, edgecolors=\"k\", cmap=plt.cm.coolwarm)\n",
    "plt.xlabel(X.columns[3])\n",
    "plt.ylabel(X.columns[4])\n",
    "plt.title(\"Decision Tree Decision Boundaries (SibSp and Parch)\")\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Not Survived\", \"Survived\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see where the model chose to divide the data into the different classes based on the features of SibSp and Parch. Each color of the background shows a region that it identifed as most inputs that meet that given criteria will have this target.\n",
    "\n",
    "Let's check out the point at (3, 0). This point in the feature space means this person had 3 siblings/spouse(s)(?) on board, and 0 parents/children on board. Our model has learned that generally anyone who had more than 2.5 siblings/spouses on board would survive. \n",
    "\n",
    "This logically also makes sense as someone in this position wouldn't have children to worry about getting off first, and they would have other adults to help them get off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, I would highly encourage you to try out swapping the normal decision tree model from SKLearn with some others. Maybe try a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) or an [XGBoost](https://xgboost.readthedocs.io/en/stable/) model, see if you can get better results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
